# -*- coding: utf-8 -*-
"""ques_answer_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjvqKwM4Wu31od6F77lLGXsQpK1LzqIc

Om Muruga
"""

import os
import streamlit as st
import pdfplumber
from langchain.llms import OpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma, Pinecone
from langchain.chains.question_answering import load_qa_chain
import pinecone

# LLM chain

llm = OpenAI(temperature=0)
chain = load_qa_chain(llm, chain_type="stuff", verbose=False)
embeddings = OpenAIEmbeddings()

# Initializing pinecone vectorstore
api_key = os.getenv("PINECONE_API_KEY")
pinecone_environment = os.getenv("PINECONE_API_ENV")
pinecone.init(
	api_key=api_key,
	environment=pinecone_environment,
)
index = pinecone.Index('doc-index')
index_name = 'doc-index'

# Page Configuration

st.set_page_config(page_title="Question-Answering System",
                            layout="centered",
                            initial_sidebar_state="expanded",
                            )

# Header

st.header("Book Question-Answering System")

# Columns

col1, col2 = st.columns(2, gap="small")
query = ""
doc_data=[]
doc_search=[]
with col1:
    st.markdown("""Loads a pdf book, splits it up into documents using LangChain,
                            gets vectors for these documents using OpenAI Embeddings,
                            stores these vectors in an external Vector Store - Pinecone -
                            and queries the book using OpenAI.\n\n*Powered by OpenAI gpt4*"""
                        )
with col1:
    uploaded_file = st.file_uploader("Upload a pdf book", type=["pdf"], key="pdf_file")
    if uploaded_file is not None:
        try:
            loader = PDFPlumberLoader(uploaded_file.name)
            doc_data = loader.load()
            # Document search
            doc_search = Pinecone.from_texts([text.page_content for text in doc_data], embeddings, index_name=index_name)

        except Exception as e:
                st.write("The file could not be loaded")
                st.write(e)

with col2:
    if len(doc_data) > 0:
        query = st.text_input(label="Query", placeholder="Enter the question...")
        if query:
            with st.spinner(text='Please Wait...'):
                try:
                    docs = doc_search.similarity_search(query)
                    answer = chain.run(input_documents=docs, question=query)
                    st.markdown("**Answer:**")
                    st.write(answer)
                except Exception as e:
                    st.write(f"Exception: {e}")
                    st.write("Could not generate answer because of the above Exception")